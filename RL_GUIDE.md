# å¼ºåŒ–å­¦ä¹ æ¨¡å—ä½¿ç”¨æŒ‡å—

## ğŸ“š æ¦‚è¿°

æœ¬é¡¹ç›®æ–°å¢äº†å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰æ¨¡å—ï¼Œå…è®¸Agenté€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜äº¤æ˜“ç­–ç•¥ï¼Œè€Œä¸æ˜¯ä¾èµ–é¢„è®¾çš„è§„åˆ™ã€‚

## ğŸ¯ æ ¸å¿ƒç»„ä»¶

### 1. TradingEnv - äº¤æ˜“ç¯å¢ƒ

ç¬¦åˆGymæ¥å£çš„äº¤æ˜“ç¯å¢ƒï¼Œå®šä¹‰äº†ï¼š
- **çŠ¶æ€ç©ºé—´**ï¼šæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ï¼ˆä»·æ ¼ã€æ”¶ç›Šç‡ã€ç§»åŠ¨å¹³å‡ã€RSIã€MACDç­‰ï¼‰
- **åŠ¨ä½œç©ºé—´**ï¼š0=ç©ºä»“, 1=æ»¡ä»“ï¼ˆå¯æ‰©å±•ä¸ºå¤šæ¡£ä½ï¼‰
- **å¥–åŠ±å‡½æ•°**ï¼šåŸºäºæ”¶ç›Šã€å¤æ™®æ¯”ç‡æˆ–é£é™©è°ƒæ•´æ”¶ç›Š

### 2. RL Trainer - è®­ç»ƒæ¨¡å—

æ”¯æŒå¤šç§RLç®—æ³•ï¼š
- **PPO** (Proximal Policy Optimization) - æ¨èï¼Œç¨³å®šé«˜æ•ˆ
- **A2C** (Advantage Actor-Critic) - å¿«é€Ÿè®­ç»ƒ
- **DQN** (Deep Q-Network) - é€‚åˆç¦»æ•£åŠ¨ä½œ
- **SAC** (Soft Actor-Critic) - é€‚åˆè¿ç»­åŠ¨ä½œ
- **TD3** (Twin Delayed DDPG) - é€‚åˆè¿ç»­åŠ¨ä½œ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install stable-baselines3[extra] gymnasium
```

æˆ–è€…å®‰è£…æ‰€æœ‰ä¾èµ–ï¼š

```bash
pip install -r requirements.txt
```

### åŸºæœ¬ä½¿ç”¨

```python
from quant_agent import DataConfig
from quant_agent.rl_trainer import train_rl_agent, evaluate_rl_agent
import datetime as dt

# 1. é…ç½®è®­ç»ƒæ•°æ®
train_data = DataConfig(
    symbol="AAPL",
    start=dt.date(2020, 1, 1),
    end=dt.date(2022, 1, 1),
)

# 2. è®­ç»ƒAgent
model, info = train_rl_agent(
    data_cfg=train_data,
    algorithm="PPO",
    total_timesteps=50000,
    model_save_path="models/rl_agent.zip",
)

# 3. è¯„ä¼°Agent
test_data = DataConfig(
    symbol="AAPL",
    start=dt.date(2022, 1, 1),
    end=dt.date(2023, 1, 1),
)

results = evaluate_rl_agent(
    model=model,
    data_cfg=test_data,
)

print(f"å¹³å‡æ”¶ç›Šç‡: {results['mean_return']:.2%}")
```

### è¿è¡Œç¤ºä¾‹

```bash
python examples_rl_training.py
```

## ğŸ“Š ç¯å¢ƒç‰¹æ€§

### çŠ¶æ€ç‰¹å¾

ç¯å¢ƒæä¾›ä¸°å¯Œçš„å¸‚åœºç‰¹å¾ï¼š
- ä»·æ ¼å½’ä¸€åŒ–
- å†å²æ”¶ç›Šç‡ï¼ˆæœ€è¿‘Nå¤©ï¼‰
- ç§»åŠ¨å¹³å‡çº¿ï¼ˆ5, 10, 20, 60å¤©ï¼‰
- Z-scoreï¼ˆå‡å€¼å›å½’æŒ‡æ ‡ï¼‰
- RSIï¼ˆç›¸å¯¹å¼ºå¼±æŒ‡æ ‡ï¼‰
- MACDï¼ˆç§»åŠ¨å¹³å‡æ”¶æ•›æ•£åº¦ï¼‰
- æ³¢åŠ¨ç‡
- å½“å‰ä»“ä½å’Œæƒç›Š

### å¥–åŠ±å‡½æ•°

æ”¯æŒä¸‰ç§å¥–åŠ±ç±»å‹ï¼š

1. **return** - ç®€å•æ”¶ç›Šå¥–åŠ±
   ```python
   reward = return_rate * 100
   ```

2. **sharpe** - åŸºäºå¤æ™®æ¯”ç‡çš„å¥–åŠ±ï¼ˆæ¨èï¼‰
   ```python
   reward = sharpe_ratio * 10
   ```

3. **risk_adjusted** - é£é™©è°ƒæ•´æ”¶ç›Š
   ```python
   reward = (return - risk_penalty) * 100
   ```

## ğŸ”§ é«˜çº§é…ç½®

### è‡ªå®šä¹‰ç¯å¢ƒå‚æ•°

```python
from quant_agent.rl_env import TradingEnv
from quant_agent.data import download_ohlcv, DataConfig

data_cfg = DataConfig(symbol="AAPL", start=dt.date(2020, 1, 1))
df = download_ohlcv(data_cfg)

env = TradingEnv(
    df=df,
    initial_cash=100000.0,
    fee_rate=0.0005,
    max_position=1.0,
    lookback_window=20,
    reward_type="sharpe",
)
```

### è®­ç»ƒå‚æ•°è°ƒä¼˜

```python
model, info = train_rl_agent(
    data_cfg=train_data,
    algorithm="PPO",
    total_timesteps=100000,  # å¢åŠ è®­ç»ƒæ­¥æ•°
    initial_cash=100000.0,
    fee_rate=0.0005,
    max_position=0.8,  # é™åˆ¶æœ€å¤§ä»“ä½
    reward_type="sharpe",
    eval_data_cfg=eval_data,  # ä½¿ç”¨éªŒè¯é›†
    verbose=1,
)
```

## ğŸ“ˆ è®­ç»ƒæŠ€å·§

### 1. æ•°æ®åˆ†å‰²

- **è®­ç»ƒé›†**ï¼šç”¨äºè®­ç»ƒæ¨¡å‹ï¼ˆå¦‚2018-2021ï¼‰
- **éªŒè¯é›†**ï¼šç”¨äºè°ƒå‚å’Œæ—©åœï¼ˆå¦‚2021-2022ï¼‰
- **æµ‹è¯•é›†**ï¼šç”¨äºæœ€ç»ˆè¯„ä¼°ï¼ˆå¦‚2022-2023ï¼‰

### 2. è®­ç»ƒæ­¥æ•°

- åˆå­¦è€…ï¼š10,000-50,000æ­¥
- ä¸­ç­‰ï¼š50,000-200,000æ­¥
- é«˜çº§ï¼š200,000+æ­¥

### 3. ç®—æ³•é€‰æ‹©

- **PPO**ï¼šé€šç”¨æ¨èï¼Œç¨³å®šé«˜æ•ˆ
- **A2C**ï¼šå¿«é€Ÿè®­ç»ƒï¼Œé€‚åˆå¿«é€Ÿå®éªŒ
- **DQN**ï¼šé€‚åˆç¦»æ•£åŠ¨ä½œç©ºé—´
- **SAC/TD3**ï¼šé€‚åˆè¿ç»­åŠ¨ä½œç©ºé—´ï¼ˆéœ€è¦ä¿®æ”¹ç¯å¢ƒï¼‰

### 4. å¥–åŠ±å‡½æ•°é€‰æ‹©

- **sharpe**ï¼šæ¨èï¼Œå¹³è¡¡æ”¶ç›Šå’Œé£é™©
- **return**ï¼šç®€å•ç›´æ¥ï¼Œä½†å¯èƒ½è¿‡åº¦å†’é™©
- **risk_adjusted**ï¼šæ›´ä¿å®ˆï¼Œé€‚åˆé£é™©åŒæ¶

## ğŸ“ æœ€ä½³å®è·µ

1. **æ•°æ®è´¨é‡**ï¼šç¡®ä¿æ•°æ®å®Œæ•´ï¼Œå¤„ç†ç¼ºå¤±å€¼
2. **ç‰¹å¾å·¥ç¨‹**ï¼šå¯ä»¥æ·»åŠ æ›´å¤šæŠ€æœ¯æŒ‡æ ‡
3. **è¶…å‚æ•°è°ƒä¼˜**ï¼šè°ƒæ•´å­¦ä¹ ç‡ã€ç½‘ç»œç»“æ„ç­‰
4. **è¿‡æ‹Ÿåˆæ£€æµ‹**ï¼šç›‘æ§è®­ç»ƒé›†å’ŒéªŒè¯é›†è¡¨ç°å·®å¼‚
5. **å›æµ‹éªŒè¯**ï¼šåœ¨æµ‹è¯•é›†ä¸Šä¸¥æ ¼è¯„ä¼°

## ğŸ” æ•…éšœæ’é™¤

### é—®é¢˜1ï¼šè®­ç»ƒä¸æ”¶æ•›

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å¢åŠ è®­ç»ƒæ­¥æ•°
- è°ƒæ•´å­¦ä¹ ç‡
- å°è¯•ä¸åŒçš„å¥–åŠ±å‡½æ•°
- æ£€æŸ¥æ•°æ®è´¨é‡

### é—®é¢˜2ï¼šæ¨¡å‹è¡¨ç°ä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®
- å¢åŠ è®­ç»ƒæ­¥æ•°
- ä½¿ç”¨éªŒè¯é›†è¿›è¡Œæ—©åœ
- å°è¯•ä¸åŒçš„ç®—æ³•

### é—®é¢˜3ï¼šå†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å‡å°‘lookback_window
- å‡å°‘ç‰¹å¾æ•°é‡
- ä½¿ç”¨è¾ƒå°çš„ç½‘ç»œç»“æ„

## ğŸ“š æ‰©å±•æ–¹å‘

1. **è¿ç»­åŠ¨ä½œç©ºé—´**ï¼šæ”¯æŒè¿ç»­ä»“ä½ï¼ˆ0-1ä¹‹é—´ä»»æ„å€¼ï¼‰
2. **å¤šæ ‡çš„äº¤æ˜“**ï¼šæ‰©å±•åˆ°æŠ•èµ„ç»„åˆç®¡ç†
3. **å¤šæ—¶é—´æ¡†æ¶**ï¼šç»“åˆæ—¥çº¿ã€å‘¨çº¿ã€æœˆçº¿æ•°æ®
4. **æ–°é—»/äº‹ä»¶**ï¼šé›†æˆæ–°é—»æƒ…æ„Ÿåˆ†æ
5. **å…ƒå­¦ä¹ **ï¼šå¿«é€Ÿé€‚åº”æ–°å¸‚åœºç¯å¢ƒ

## ğŸ”— å‚è€ƒèµ„æº

- [Stable-Baselines3 æ–‡æ¡£](https://stable-baselines3.readthedocs.io/)
- [Gymnasium æ–‡æ¡£](https://gymnasium.farama.org/)
- ã€Šå¼ºåŒ–å­¦ä¹ ï¼šåŸç†ä¸Pythonå®ç°ã€‹
- ã€Šæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼šåŸç†ä¸å®è·µã€‹

---

**æ³¨æ„**ï¼šå¼ºåŒ–å­¦ä¹ è®­ç»ƒéœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œå»ºè®®åœ¨GPUä¸Šè®­ç»ƒä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

