# 简历展示Demo使用指南

## 📋 概述

`resume_demo.py` 是一个综合性的演示脚本，展示量化交易Agent项目的核心功能，适合用于简历展示和技术面试。

## 🚀 快速运行

```bash
python resume_demo.py
```

## 📊 Demo内容

### Demo 1: 传统策略回测对比
- 对比均值回归和动量策略
- 展示收益率、夏普比率、最大回撤等指标
- 生成净值曲线对比图

### Demo 2: 市场状态识别和动态策略选择
- 自动识别市场状态（趋势市、震荡市等）
- 根据市场状态推荐最优策略
- 展示智能决策能力

### Demo 3: 智能仓位管理
- 凯利公式计算最优仓位
- 风险平价仓位管理
- 展示风险管理能力

### Demo 4: 自动参数优化
- 网格搜索寻找最优参数
- 展示自动化优化能力

### Demo 5: 强化学习训练（可选）
- 使用PPO算法训练RL Agent
- 展示机器学习应用能力

## 📁 输出文件

运行后会生成：
- `demo_output/strategy_comparison.png` - 策略对比图
- `demo_output/rl_agent_demo.zip` - RL模型（如果运行了RL demo）

## 💡 简历展示建议

### 1. 技术亮点
- **智能Agent架构**：感知-决策-执行-评估循环
- **多策略框架**：均值回归、动量、强化学习
- **市场状态识别**：自动识别并适应市场变化
- **智能仓位管理**：凯利公式、风险平价
- **自动化优化**：参数网格搜索

### 2. 项目成果
- 实现了完整的量化交易框架
- 支持多市场（美股、A股、港股、加密货币）
- 集成传统策略和强化学习
- 完整的回测和评估系统

### 3. 技术栈
- Python, pandas, numpy
- stable-baselines3 (强化学习)
- gymnasium (RL环境)
- matplotlib (可视化)

## 🎯 面试准备

### 可能的问题和回答

**Q: 为什么选择这些策略？**
A: 均值回归适合震荡市，动量策略适合趋势市。通过市场状态识别，可以动态选择最优策略。

**Q: 强化学习相比传统策略有什么优势？**
A: RL可以学习复杂的市场模式，适应不同市场环境，不需要预设规则。但需要更多数据和计算资源。

**Q: 如何避免过拟合？**
A: 使用滚动窗口优化、样本外测试、交叉验证等方法。在RL训练中使用验证集进行早停。

**Q: 仓位管理的重要性？**
A: 好的仓位管理可以显著提升策略表现。我们实现了凯利公式和风险平价，根据风险和收益动态调整仓位。

## 📝 运行时间

- Demo 1-4: 约1-3分钟
- Demo 5 (RL): 约5-10分钟（取决于timesteps）

## ⚠️ 注意事项

1. 首次运行需要下载数据，可能需要一些时间
2. RL训练需要安装 `stable-baselines3`，如果未安装会跳过
3. 确保有足够的磁盘空间存储数据和模型

## 🔧 自定义

可以修改以下参数：
- 股票代码（symbol）
- 时间范围（start, end）
- 训练步数（RL timesteps）
- 参数网格（优化范围）

---

**提示**：运行前确保已安装所有依赖：`pip install -r requirements.txt`

